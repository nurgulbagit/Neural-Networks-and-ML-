{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurgulbagit/Neural-Networks-and-ML-/blob/main/Copy_of_MIS382N_HW1_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIS382N: Homework 1\n",
        "\n",
        "Submit:\n",
        "\n",
        "A read-only link to your colab notebook, executed and saved (so that we can run it, but don't necessarily have to, to see your results).\n",
        "\n",
        "For any questions that require written explanation, please use Markdown - it is a lot easier for us to read than comments in the code.\n",
        "\n",
        "**If you used any AI help, please describe how you used it and what you used.**\n",
        "\n",
        "**Goals of this Lab:**\n",
        "1.  Practice with Pandas, Numpy and Data Exploration.\n",
        "2.  Training vs testing error\n",
        "3.  Review important results from probability, such as the CLT.\n"
      ],
      "metadata": {
        "id": "cODeBzg1alZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some useful libraries\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "#Pandas for data structure and analysis tools\n",
        "import pandas as pd\n",
        "\n",
        "#seaborn and matplotlib for plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#for nice vector graphics\n",
        "%matplotlib inline\n",
        "\n",
        "#from IPython.display import set_matplotlib_formats\n",
        "#set_matplotlib_formats('png', 'pdf')\n",
        "\n",
        "np.random.seed(42) # Fixed seed for reproducibility\n",
        "rng = default_rng(42)"
      ],
      "metadata": {
        "id": "4Xvcqq7SapU5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1\n",
        "\n",
        "Download from Canvas/Files the dataset ``PatientData.csv``. Each row is a patient and the last column is the condition that the patient has.  Do data exploration using Pandas and other visualization tools to understand what you can about the data set. For example:\n",
        "\n",
        "Part 1.  How many patients and how many features are there?\n",
        "\n",
        "Part 2.  What is the meaning of the first 4 features?  See if you can understand what they mean.\n",
        "\n",
        "Part 3.  Are there missing values?  Replace them with the average of the corresponding feature column\n",
        "\n",
        "Part 4.  How could you test which features strongly influence the patient condition and which do not? List what you think are the three most important features."
      ],
      "metadata": {
        "id": "OG1S6pFANq4b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PAduBHNrdT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2\n",
        "\n",
        "We looked at the MNIST data set in class (see the notebook in ``pages`` in Canvas). Recall that MNIST is a data set of handwritten digits. It is considered one of the ``easiest'' image recognition problems in computer vision, and we will see it again.\n",
        "\n",
        "This exercise is simple exercise, and an opportunity to play around with this data set.\n",
        "\n",
        "**Parts 1 to 7: Nothing to submit!** just run through the notebook we saw in class and try to make sure you understand all the commands.\n",
        "\n",
        "Part 1. Make sure you can run through the entire Colab notebook posted. Especially if you haven't used Python, try to understand what every line is doing.\n",
        "\n",
        "Part 2. How many data points are there, how many features are there, and what do the features represent?\n",
        "\n",
        "Part 3. Compute how many times each digit appears in the dataset.\n",
        "\n",
        "Part 4. Read the documentation for sklearn.model_selection.train_test_split and explain what this does.\n",
        "\n",
        "Part 5. Read the documentation for DecisionTreeClassifier, and explain what score means.\n",
        "\n",
        "Part 6. What happens to the **training score** as you increase the depth of the tree? Explain.\n",
        "\n",
        "Part 7. What happens to the difference between **training score** and **testing score** as you increase the depth of the tree? Explain.\n",
        "\n",
        "**Only submit your answer to this part**\n",
        "\n",
        "Part 8. Fix the depth of the tree. For this example, use depth=6 (nothing special about the number 6, just to illustrate the concept). Then plot the difference of training score - testing score when you train on: 100, 500, 5000, 10000, 20,0000, 60,000 training points, always computing testing score by evaluating on the 10,000 testing points. Plot this trend.  Try to explain what you are seeing.  \n"
      ],
      "metadata": {
        "id": "01vCW44LTMC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3\n",
        "\n",
        "We now turn to a somewhat more sophisticated data set: CIFAR10.\n",
        "\n",
        "Load the data using:\n",
        "```\n",
        "from keras.datasets import cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "```\n",
        "\n",
        "Part 1. How many data points are there, and how many labels? How many points for each label?\n",
        "\n",
        "Part 2. Figure out how to display some of the images from CIFAR10.\n",
        "\n",
        "Part 3: Reshape X_train into a matrix X_tr so that each row is a data point.\n",
        " What are the dimensions of X_train?\n",
        "\n",
        "Part 4: Generate decision trees of depth 1 to 15, and for each, compute the training accuracy and the testing accuracy.\n",
        "\n",
        "If you did this correctly and ran the notebook, you noticed that CIFAR10 indeed looks like a ``harder'' problem. Deep trees are again doing very well on the training set, and they do a little better than guessing on the testing data, but not as well as they do on MNIST. We will revisit CIFAR10 several times, as we develop more powerful tools. And we will see that we will do much better than deep decision trees!"
      ],
      "metadata": {
        "id": "naZRl9cbmYbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The next two problems are optional, but recommended\n",
        "\n",
        "We will be generating lots of random data sets in this class, in order to test the behavior of machine learning algorithms.\n",
        "\n",
        "So it's useful to know how to generate random data, and display it."
      ],
      "metadata": {
        "id": "yQ7dZ0zOubdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4 (Optional, recommended)\n",
        "\n",
        "**Part 1.** Generate 1,000 samples of 2 dimensional data from the Gaussian distribution $\\left(\\begin{array}{cc}X_{i}\\\\Y_{i}\\end{array}\\right)âˆ¼N\\biggl(\\left(\\begin{array}{cc}-5\\\\5\\end{array}\\right),\\left(\\begin{array}{cc}2 & 0.8\\\\0.8 & 3\\end{array}\\right)\\biggr)$.\n",
        "\n",
        "**Part 2.** Plot these points.\n",
        "\n",
        "**Part 3.** Find the Eigenvectors and Eigenvalues of the covariance matrix using np.linalg.eig, or np.linalg.eigh, or something else of your choice.\n",
        "\n",
        "**Part 4.** Now take the 1,000 points you generated in the first part, and use them to estimate the mean and covariance matrix for this multi-dimensional data using elementary numpy commands, i.e., addition, multiplication, division (do not use a command that takes data and returns the mean or standard deviation).\n",
        "\n",
        "*Remark*: If you did this correctly: You should have made a number of observations. (i) The points you plotted should look like an elongated ellipse. (ii) The axis of elongation (the major axis of the ellipse) should be aligned with the eigenvector you computed that has the largest eigenvalue. The minor axis, should be aligned with the other eigenvector you computed. (iii) In the last part, you computed what is called the *empirical covariance* matrix. This should be quite close to the covariance matrix you used to generate the data. If we used more and more points (10,000, 100,000, etc.), then our empirical estimate would look more and more like what we used to generate the data.\n"
      ],
      "metadata": {
        "id": "2cb_GbUlauEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 5: Central Limit Theorem (Optional, recommended)\n",
        "\n",
        "Back in your probability/statistics class, you learned the Law of Large Numbers, and the Central Limit Theorem, among many other things. The Law of Large Numbers says that if $X_i$ are independent and identically distributed (iid) random variables, then $(1/N) \\sum X_i$ converges to $\\mathbb{E}[X]$. That's the law of large numbers.\n",
        "\n",
        "You also learned the Central Limit Theorem. This says that if $X_i$ are zero mean, have variance 1, and are iid, then $(1/\\sqrt{N}) \\sum X_i$ converges to a random variable. Which random variable? A standard (zero mean, unit variance) Gaussian.\n",
        "\n",
        "We're going to check the central limit theorem empirically, as an excuse to do more practice with Python and numpy and basic plotting.\n",
        "\n",
        "Let $X_i$ be an iid Bernoulli random variable with value \\{-1,1\\}. Look at the random variable\n",
        "$Z_n = \\frac{1}{\\sqrt{n}}\\sum X_i$. By taking 1000 samples from $Z_n$, plot its histogram. **Note**: To generate 1,000 samples from $Z_n$, you need to generate $1,000 \\times n$ samples of $X_i$, since each $Z$ needs $1,000$ $X_i$'s. Now check that for small $n$ (set $n= 5$) $Z_n$ does not look that much like a Gaussian, but when $n$ is bigger (set $n = 50$) it looks much more like a Gaussian. Check also for much bigger $n$: $n = 250$, to see that at this point, one can really see the bell curve."
      ],
      "metadata": {
        "id": "Dp-Seh6FNCT7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYxJU9YInYSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}